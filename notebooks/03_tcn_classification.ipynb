{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1525a388",
   "metadata": {},
   "source": [
    "## 3. Temporal Convolutional Network (TCN) Classification\n",
    "\n",
    "This section implements the Temporal Convolutional Network (TCN) classifier\n",
    "used in the original study to model temporal dependencies in sequences of\n",
    "dynamic functional connectivity features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99079374",
   "metadata": {},
   "source": [
    "### Expected input shape\n",
    "\n",
    "Each subject is represented as a sequence of T dynamic functional\n",
    "connectivity vectors of dimensionality 6670, i.e., a tensor of shape\n",
    "(T, 6670), where T is fixed per acquisition site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8301066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaborca/Documents/AI_2023-2026/Semestrul_5/KBS/Report_3/adhd-tcn-replication/.venv/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def tcn_residual_block(x, filters=3, kernel_size=3, dilation=1, dropout=0.3, name=\"res\"):\n",
    "    shortcut = x\n",
    "\n",
    "    # 1st dilated causal conv\n",
    "    y = layers.Conv1D(filters, kernel_size, padding=\"causal\",\n",
    "                      dilation_rate=dilation, use_bias=True,\n",
    "                      name=f\"{name}_conv1\")(x)\n",
    "    y = layers.BatchNormalization(name=f\"{name}_bn1\")(y)\n",
    "    y = layers.ReLU(name=f\"{name}_relu1\")(y)\n",
    "    y = layers.Dropout(dropout, name=f\"{name}_drop1\")(y)\n",
    "\n",
    "    # 2nd dilated causal conv (same dilation)\n",
    "    y = layers.Conv1D(filters, kernel_size, padding=\"causal\",\n",
    "                      dilation_rate=dilation, use_bias=True,\n",
    "                      name=f\"{name}_conv2\")(y)\n",
    "    y = layers.BatchNormalization(name=f\"{name}_bn2\")(y)\n",
    "    y = layers.ReLU(name=f\"{name}_relu2\")(y)\n",
    "    y = layers.Dropout(dropout, name=f\"{name}_drop2\")(y)\n",
    "\n",
    "    # Match channels for residual add if needed\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding=\"same\", use_bias=True,\n",
    "                                 name=f\"{name}_proj\")(shortcut)\n",
    "\n",
    "    out = layers.Add(name=f\"{name}_add\")([shortcut, y])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850070d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdnet_tcn_classifier(T, F=6670, filters=3, kernel_size=3, dropout=0.3):\n",
    "    \"\"\"\n",
    "    Paper-faithful TDNet temporal module + fusion + classifier.\n",
    "    Input:  (T, 6670)\n",
    "    Output: (2,) softmax\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(T, F), name=\"H_seq\")  # H ∈ R^{T×6670}\n",
    "\n",
    "    # TCN: Q=3 residual blocks, dilations = 1,2,4\n",
    "    x = inp\n",
    "    x = tcn_residual_block(x, filters=filters, kernel_size=kernel_size, dilation=1, dropout=dropout, name=\"tcn_b1\")\n",
    "    x = tcn_residual_block(x, filters=filters, kernel_size=kernel_size, dilation=2, dropout=dropout, name=\"tcn_b2\")\n",
    "    x = tcn_residual_block(x, filters=filters, kernel_size=kernel_size, dilation=4, dropout=dropout, name=\"tcn_b3\")\n",
    "\n",
    "    HQ = x  # shape: (B, T, D) where D=filters=3\n",
    "\n",
    "    # 1×1 conv over HQ with tanh to get P ∈ R^T (per sample)\n",
    "    P = layers.Conv1D(1, 1, padding=\"same\", activation=\"tanh\", name=\"time_weight_conv\")(HQ)  # (B,T,1)\n",
    "\n",
    "    # Fuse: H' = P H  -> (B, 6670)\n",
    "    # reshape P to (B,1,T) and matmul with H (B,T,F) -> (B,1,F) -> squeeze -> (B,F)\n",
    "    P_row = layers.Permute((2,1), name=\"P_row\")(P)  # (B,1,T)\n",
    "    H_fused = layers.Lambda(lambda z: tf.matmul(z[0], z[1]), name=\"fuse_matmul\")([P_row, inp])  # (B,1,F)\n",
    "    H_fused = layers.Lambda(lambda z: tf.squeeze(z, axis=1), name=\"fuse_squeeze\")(H_fused)      # (B,F)\n",
    "\n",
    "    # Classifier: 512 -> 128 -> 2 (softmax)\n",
    "    x = layers.Dense(512, activation=\"relu\", name=\"fc1\")(H_fused)\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"fc2\")(x)\n",
    "    out = layers.Dense(2, activation=\"softmax\", name=\"fc3\")(x)\n",
    "\n",
    "    return Model(inp, out, name=f\"TDNet_TCN_T{T}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ea0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tdnet_tcn_classifier(T=8)  # example: NYU\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),   # default params\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
