{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a294cd8",
   "metadata": {},
   "source": [
    "## 2.1 Partition of rs-fMRI time series\n",
    "\n",
    "We follow the paper’s partition strategy: window length L=20 timepoints.\n",
    "To ensure site-consistent window counts, each site is truncated to the\n",
    "time-series length reported in the paper (KKI 119, NYU 171, OHSU 74,\n",
    "NeuroIMAGE 257, Peking_1 231). Subjects shorter than the target length\n",
    "are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "469a66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "ArrayLike = Union[np.ndarray]\n",
    "\n",
    "@dataclass\n",
    "class WindowingResult:\n",
    "    windows: np.ndarray  # shape: (T, N, L) \n",
    "    T: int               # number of valid windows \n",
    "    dropped: int         # number of dropped timepoints at the end (0...L-1)\n",
    "    L: int               # window length\n",
    "    step: int            # step size between windows\n",
    "    N: int               # number of ROIs\n",
    "    M: int               # original number of timepoints\n",
    "\n",
    "\n",
    "def partition_time_series(\n",
    "    X: ArrayLike,\n",
    "    L: int = 20,\n",
    "    *,\n",
    "    overlap: Optional[int] = None,\n",
    "    step: Optional[int] = None,\n",
    "    time_axis: int = -1,\n",
    "    drop_incomplete: bool = True,\n",
    "    return_metadata: bool = True,\n",
    ") -> Union[np.ndarray, WindowingResult]:\n",
    "    \"\"\"\n",
    "    Sliding-window partitioning of ROI time series.\n",
    "\n",
    "    X shape:\n",
    "      - (N, M) if time_axis=-1  (default)\n",
    "      - (M, N) if time_axis=0\n",
    "\n",
    "    Params:\n",
    "      - L: window length (timepoints)\n",
    "      - overlap: number of timepoints overlapped between consecutive windows (0..L-1)\n",
    "      - step: hop size between window starts (1..L). If provided, it overrides overlap.\n",
    "      - drop_incomplete: if True, only keep full windows of length L (paper behavior)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"X must be 2D, got shape {X.shape}\")\n",
    "    if L <= 0:\n",
    "        raise ValueError(\"L must be positive\")\n",
    "    \n",
    "    if time_axis == 0:\n",
    "        X_nt = X.T\n",
    "    elif time_axis == -1:\n",
    "        X_nt = X\n",
    "    else:\n",
    "        raise ValueError(\"time_axis must be 0 (time first) or -1 (time last)\")\n",
    "    \n",
    "    N, M = X_nt.shape\n",
    "\n",
    "    if step is None:\n",
    "        if overlap is None:\n",
    "            step = L  # no overlap\n",
    "        else:\n",
    "            if not (0 <= overlap < L):\n",
    "                raise ValueError(f\"overlap must be in [0, {L-1}], got {overlap}\")\n",
    "            step = L - overlap\n",
    "    else:\n",
    "        if not (1 <= step <= L):\n",
    "            raise ValueError(f\"step must be in [1, {L}], got {step}\")\n",
    "        \n",
    "    # Compute window start indices\n",
    "    starts = np.arange(0, M, step, dtype=int)\n",
    "    if drop_incomplete:\n",
    "        starts = starts[starts + L <= M]\n",
    "\n",
    "    T = len(starts)\n",
    "    if T == 0:\n",
    "        windows = np.empty((0, N, L), dtype=X.dtype)\n",
    "        dropped = M\n",
    "        return WindowingResult(windows, T, dropped, L, step, N, M) if return_metadata else windows\n",
    "    \n",
    "    windows = np.stack([X_nt[:, s:s+L] for s in starts], axis=0)  # shape (T, N, L)\n",
    "\n",
    "    # Define \"dropped\" as the tail after the last possible full-window start\n",
    "    last_start = starts[-1]\n",
    "    used_until = last_start + L\n",
    "    dropped = max(0, M - used_until)\n",
    "\n",
    "    if return_metadata:\n",
    "        return WindowingResult(windows, T, dropped, L, step, N, M)\n",
    "    else:\n",
    "        return windows\n",
    "\n",
    "def partition_dataset(\n",
    "    subjects: List[np.ndarray],\n",
    "    L: int = 20,\n",
    "    time_axis: int = -1,\n",
    ") -> Tuple[List[np.ndarray], List[int]]:\n",
    "    \"\"\"\n",
    "    Apply partitioning to a list of subjects.\n",
    "\n",
    "    Returns:\n",
    "      - windows_per_subject: List of arrays, each (T_i, N, L)\n",
    "      - T_per_subject: List[int], number of windows per subject\n",
    "    \"\"\"\n",
    "    windows_per_subject = []\n",
    "    T_per_subject = []\n",
    "    for subj_ts in subjects:\n",
    "        result = partition_time_series(subj_ts, L=L, time_axis=time_axis, return_metadata=True)\n",
    "        windows_per_subject.append(result.windows)\n",
    "        T_per_subject.append(result.T)\n",
    "    return windows_per_subject, T_per_subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9801a",
   "metadata": {},
   "source": [
    "### Loading Athena ROI time series (.1D)\n",
    "Athena provides AAL ROI mean time series in AFNI-style .1D files with header\n",
    "columns (e.g., `File`, `Sub-brick`, and `Mean_*` ROI columns). We parse these\n",
    "files and keep only `Mean_*` columns, yielding a timepoints × ROIs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cf524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 511 | cols: ['site', 'subject_id', 'tc_path', 'T', 'R', 'DX', 'dx_raw', 'label']\n",
      "Test rows : 162 | cols: ['site', 'subject_id', 'tc_path', 'T', 'R', 'DX']\n",
      "Inferred PATH_COL = tc_path\n",
      "\n",
      "=== TRAIN | non-overlapping (L=20, step=20) ===\n",
      "\n",
      "Site: NYU\n",
      "  subjects kept: 216\n",
      "  M (timepoints): min=171, median=171, max=171\n",
      "  T (windows, L=20, step=20): min=8, median=8, max=8\n",
      "  dropped tail: min=11, median=11, max=11\n",
      "\n",
      "Site: NeuroIMAGE\n",
      "  subjects kept: 48\n",
      "  M (timepoints): min=257, median=257, max=257\n",
      "  T (windows, L=20, step=20): min=12, median=12, max=12\n",
      "  dropped tail: min=17, median=17, max=17\n",
      "\n",
      "Site: KKI\n",
      "  subjects kept: 83\n",
      "  M (timepoints): min=119, median=119, max=119\n",
      "  T (windows, L=20, step=20): min=5, median=5, max=5\n",
      "  dropped tail: min=19, median=19, max=19\n",
      "\n",
      "Site: Peking_1\n",
      "  subjects kept: 85\n",
      "  M (timepoints): min=231, median=231, max=231\n",
      "  T (windows, L=20, step=20): min=11, median=11, max=11\n",
      "  dropped tail: min=11, median=11, max=11\n",
      "\n",
      "Site: OHSU\n",
      "  subjects kept: 78\n",
      "  dropped (shorter than paper M): 1\n",
      "  M (timepoints): min=74, median=74, max=74\n",
      "  T (windows, L=20, step=20): min=3, median=3, max=3\n",
      "  dropped tail: min=14, median=14, max=14\n",
      "\n",
      "=== TEST | non-overlapping (L=20, step=20) ===\n",
      "\n",
      "Site: KKI\n",
      "  subjects kept: 11\n",
      "  M (timepoints): min=119, median=119, max=119\n",
      "  T (windows, L=20, step=20): min=5, median=5, max=5\n",
      "  dropped tail: min=19, median=19, max=19\n",
      "\n",
      "Site: NYU\n",
      "  subjects kept: 41\n",
      "  M (timepoints): min=171, median=171, max=171\n",
      "  T (windows, L=20, step=20): min=8, median=8, max=8\n",
      "  dropped tail: min=11, median=11, max=11\n",
      "\n",
      "Site: NeuroIMAGE\n",
      "  subjects kept: 20\n",
      "  dropped (shorter than paper M): 5\n",
      "  M (timepoints): min=257, median=257, max=257\n",
      "  T (windows, L=20, step=20): min=12, median=12, max=12\n",
      "  dropped tail: min=17, median=17, max=17\n",
      "\n",
      "Site: Peking_1\n",
      "  subjects kept: 51\n",
      "  M (timepoints): min=231, median=231, max=231\n",
      "  T (windows, L=20, step=20): min=11, median=11, max=11\n",
      "  dropped tail: min=11, median=11, max=11\n",
      "\n",
      "Site: OHSU\n",
      "  subjects kept: 0\n",
      "  dropped (shorter than paper M): 34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---- Paths ----\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "train_csv = DATA_PROCESSED / \"subjects_train_paper.csv\"\n",
    "test_csv  = DATA_PROCESSED / \"subjects_test_paper.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test  = pd.read_csv(test_csv)\n",
    "\n",
    "print(\"Train rows:\", len(df_train), \"| cols:\", list(df_train.columns))\n",
    "print(\"Test rows :\", len(df_test),  \"| cols:\", list(df_test.columns))\n",
    "\n",
    "# ---- Find the time-series path column automatically ----\n",
    "# Common names you might have used in df_tc / manifests:\n",
    "CANDIDATE_PATH_COLS = [\n",
    "    \"tc_path\", \"timeseries_path\", \"time_series_path\", \"path\",\n",
    "    \"roi_ts_path\", \"roi_timeseries_path\", \"file\", \"filepath\",\n",
    "    \"roi_path\", \"aal_path\"\n",
    "]\n",
    "\n",
    "def infer_path_col(df: pd.DataFrame) -> str:\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in CANDIDATE_PATH_COLS:\n",
    "        if cand.lower() in cols:\n",
    "            return cols[cand.lower()]\n",
    "    # fallback: first col that contains 'path'\n",
    "    for c in df.columns:\n",
    "        if \"path\" in c.lower():\n",
    "            return c\n",
    "    raise ValueError(\n",
    "        \"Could not infer the time-series path column. \"\n",
    "        \"Please set PATH_COL manually.\"\n",
    "    )\n",
    "\n",
    "PATH_COL = infer_path_col(df_train)\n",
    "print(\"Inferred PATH_COL =\", PATH_COL)\n",
    "\n",
    "# ---- Loader for one subject time series ----\n",
    "def load_subject_timeseries(tc_path: str, expected_N: int = 116) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loads ROI time series from a path.\n",
    "    Supports: .npy, .npz, .csv, .tsv, .txt, .1D (Athena/AFNI style with headers).\n",
    "    Returns a 2D numpy array (not yet forced to N x M).\n",
    "    \"\"\"\n",
    "    p = Path(tc_path)\n",
    "    if not p.is_absolute():\n",
    "        p = (PROJECT_ROOT / p).resolve()\n",
    "\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Time-series file not found: {p}\")\n",
    "\n",
    "    suf = p.suffix.lower()\n",
    "\n",
    "    if suf == \".npy\":\n",
    "        X = np.load(p)\n",
    "        return np.asarray(X)\n",
    "\n",
    "    if suf == \".npz\":\n",
    "        z = np.load(p)\n",
    "        key0 = list(z.keys())[0]\n",
    "        X = z[key0]\n",
    "        return np.asarray(X)\n",
    "\n",
    "    # Text-like formats\n",
    "    if suf in [\".csv\"]:\n",
    "        df = pd.read_csv(p, sep=\",\")\n",
    "    elif suf in [\".tsv\"]:\n",
    "        df = pd.read_csv(p, sep=\"\\t\")\n",
    "    elif suf in [\".txt\", \".1d\"]:\n",
    "        # Athena .1D often uses tabs or variable whitespace. Python engine handles regex sep.\n",
    "        # Also tolerate comment lines.\n",
    "        df = pd.read_csv(p, sep=r\"\\s+|\\t+\", engine=\"python\", comment=\"#\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {suf} for {p}\")\n",
    "\n",
    "    # If it already came in without headers (rare), fall back to numeric conversion\n",
    "    if df.shape[1] == 1:\n",
    "        # might be because separator didn't match; try whitespace\n",
    "        df = pd.read_csv(p, sep=r\"\\s+\", engine=\"python\", comment=\"#\")\n",
    "\n",
    "    # Prefer ROI columns named like Mean_XXXX\n",
    "    mean_cols = [c for c in df.columns if str(c).startswith(\"Mean_\")]\n",
    "\n",
    "    if len(mean_cols) > 0:\n",
    "        roi_df = df[mean_cols]\n",
    "    else:\n",
    "        # Otherwise drop obvious metadata columns and keep numeric columns\n",
    "        drop_cols = [c for c in df.columns if str(c).lower() in [\"file\", \"sub-brick\", \"subbrick\", \"brick\", \"index\"]]\n",
    "        tmp = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        # Keep numeric-convertible columns\n",
    "        roi_df = tmp.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Drop rows that are all NaN (can happen if header/formatting issues)\n",
    "    roi_df = roi_df.dropna(axis=0, how=\"all\")\n",
    "\n",
    "    X = roi_df.to_numpy(dtype=float)\n",
    "\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"Loaded time series must be 2D, got shape {X.shape} from {p}\")\n",
    "\n",
    "    # Sanity check: try to ensure we got ~116 ROI columns\n",
    "    if X.shape[1] != expected_N and X.shape[0] == expected_N:\n",
    "        # could be transposed already; keep as-is\n",
    "        pass\n",
    "    elif X.shape[1] != expected_N and X.shape[0] != expected_N:\n",
    "        # Don't hard-fail; just warn so you can inspect one file quickly.\n",
    "        print(f\"[WARN] {p.name}: unexpected shape {X.shape} (expected one dim == {expected_N}).\")\n",
    "\n",
    "    return X\n",
    "\n",
    "def ensure_N_by_M(X: np.ndarray, expected_N: int = 116) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Ensures X is (N, M). If X is (M, N), transpose it.\n",
    "    Returns (X_fixed, time_axis_used).\n",
    "    \"\"\"\n",
    "    if X.shape[0] == expected_N:\n",
    "        return X, -1  # already (N, M)\n",
    "    if X.shape[1] == expected_N:\n",
    "        return X.T, 0  # was (M, N), transposed\n",
    "    raise ValueError(f\"Expected one dimension to be N={expected_N} ROIs, got shape {X.shape}\")\n",
    "\n",
    "PAPER_M = {\n",
    "    \"KKI\": 119,\n",
    "    \"NYU\": 171,\n",
    "    \"OHSU\": 74,\n",
    "    \"NeuroIMAGE\": 257,\n",
    "    \"Peking_1\": 231\n",
    "}\n",
    "\n",
    "def truncate_to_paper_length(\n",
    "    X_nm: np.ndarray,\n",
    "    site: str,\n",
    "    *,\n",
    "    drop_if_shorter: bool = True\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    X_nm shape: (N, M). Truncate to the paper's site-specific timepoints if longer.\n",
    "    If shorter than expected, keep as-is (or you can drop those subjects).\n",
    "    \"\"\"\n",
    "    target = PAPER_M.get(site)\n",
    "    if target is None:\n",
    "        return X_nm\n",
    "    \n",
    "    M = X_nm.shape[1]\n",
    "    if M >= target:\n",
    "        return X_nm[:, :target]\n",
    "    else:\n",
    "       return None if drop_if_shorter else X_nm\n",
    "\n",
    "# ---- Apply partitioning to a manifest ----\n",
    "def partition_manifest(\n",
    "    df,\n",
    "    L=20,\n",
    "    step=20,\n",
    "    overlap=None,\n",
    "    expected_N=116,\n",
    "    max_subjects=None,\n",
    "    drop_if_shorter=True,\n",
    "):\n",
    "    per_site = defaultdict(list)\n",
    "    dropped_short = defaultdict(int)\n",
    "\n",
    "    n = len(df) if max_subjects is None else min(len(df), max_subjects)\n",
    "\n",
    "    for i in range(n):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        X = load_subject_timeseries(row[PATH_COL])\n",
    "        X_fixed, _ = ensure_N_by_M(X, expected_N=expected_N)\n",
    "\n",
    "        site = row[\"site\"]\n",
    "\n",
    "        # ---- NEW: truncate to paper length ----\n",
    "        X_fixed = truncate_to_paper_length(X_fixed, site, drop_if_shorter=drop_if_shorter)\n",
    "        if X_fixed is None:\n",
    "            dropped_short[site] += 1\n",
    "            continue\n",
    "\n",
    "        res = partition_time_series(\n",
    "            X_fixed,\n",
    "            L=L,\n",
    "            step=step,\n",
    "            overlap=overlap,\n",
    "            time_axis=-1,\n",
    "            drop_incomplete=True,\n",
    "            return_metadata=True,\n",
    "        )\n",
    "\n",
    "        per_site[site].append((res.M, res.T, res.dropped))\n",
    "\n",
    "    # Print summary\n",
    "    for site, vals in per_site.items():\n",
    "        Ms = np.array([v[0] for v in vals])\n",
    "        Ts = np.array([v[1] for v in vals])\n",
    "        Ds = np.array([v[2] for v in vals])\n",
    "\n",
    "        print(f\"\\nSite: {site}\")\n",
    "        print(f\"  subjects kept: {len(vals)}\")\n",
    "        if dropped_short.get(site, 0) > 0:\n",
    "            print(f\"  dropped (shorter than paper M): {dropped_short[site]}\")\n",
    "        print(f\"  M (timepoints): min={Ms.min()}, median={int(np.median(Ms))}, max={Ms.max()}\")\n",
    "        print(f\"  T (windows, L={L}, step={step}): min={Ts.min()}, median={int(np.median(Ts))}, max={Ts.max()}\")\n",
    "        print(f\"  dropped tail: min={Ds.min()}, median={int(np.median(Ds))}, max={Ds.max()}\")\n",
    "\n",
    "    # Also show if any site had only dropped subjects (edge case)\n",
    "    for site, cnt in dropped_short.items():\n",
    "        if site not in per_site:\n",
    "            print(f\"\\nSite: {site}\")\n",
    "            print(f\"  subjects kept: 0\")\n",
    "            print(f\"  dropped (shorter than paper M): {cnt}\")\n",
    "\n",
    "    return per_site\n",
    "\n",
    "# ---- Run: non-overlapping (paper default) ----\n",
    "print(\"\\n=== TRAIN | non-overlapping (L=20, step=20) ===\")\n",
    "_ = partition_manifest(df_train, L=20, step=20)\n",
    "\n",
    "print(\"\\n=== TEST | non-overlapping (L=20, step=20) ===\")\n",
    "_ = partition_manifest(df_test, L=20, step=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8c2a0",
   "metadata": {},
   "source": [
    "Note: After enforcing paper time-series lengths, OHSU test subjects were shorter than 74 timepoints in the Athena release and were excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e403ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_PER_SITE = {\n",
    "    \"KKI\": 5,\n",
    "    \"NYU\": 8,\n",
    "    \"OHSU\": 3,\n",
    "    \"NeuroIMAGE\": 12,\n",
    "    \"Peking_1\": 11\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e74d52",
   "metadata": {},
   "source": [
    "## 2.2 Dynamic functional connectivity generation (CNN + bilinear pooling)\n",
    "\n",
    "For each window, we extract ROI-wise temporal features using a 3-layer CNN\n",
    "(Conv → BN → ReLU → Dropout). We then construct the dynamic functional\n",
    "connectivity matrix via bilinear pooling: A_t = H_t H_t^T, producing a\n",
    "symmetric N×N FC matrix. Finally, we vectorize the upper triangular part\n",
    "(excluding the diagonal) to obtain a 6670-D feature vector per window.\n",
    "\n",
    "### Expected tensor shapes (paper-faithful)\n",
    "\n",
    "Input window:        (N=116, L=20)\n",
    "\n",
    "After CNN (Keras): (N=116, d, 1) → squeeze → (N=116, d)\n",
    "With valid padding and two (1×3) conv layers: d = 16\n",
    "\n",
    "After bilinear FC:   (116, 116)\n",
    "\n",
    "Vectorized FC:       (116 * 115 / 2 = 6670)\n",
    "\n",
    "Sequence per subject (T, 6670)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e57379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def tdnet_cnn_block(N=116, L=20, dropout=0.3):\n",
    "    inp = layers.Input(shape=(N, L, 1))\n",
    "\n",
    "    x = layers.Conv2D(4, (1, 3), padding=\"valid\", use_bias=True)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Conv2D(2, (1, 3), padding=\"valid\", use_bias=True)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Conv2D(1, (1, 1), padding=\"valid\", use_bias=True)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    return Model(inp, x, name=\"TDNet_CNN_Block\")\n",
    "\n",
    "def upper_triangle_vectorize(A: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    A: (B, N, N) symmetric FC matrices\n",
    "    Returns: (B, N*(N-1)/2) upper-triangular (excluding diagonal)\n",
    "    \"\"\"\n",
    "    N = tf.shape(A)[-1]\n",
    "    # boolean mask for upper triangle excluding diagonal\n",
    "    mask = tf.linalg.band_part(tf.ones((N, N), dtype=tf.bool), 0, -1)  # upper incl diag\n",
    "    mask = tf.logical_and(mask, tf.logical_not(tf.eye(N, dtype=tf.bool)))  # exclude diag\n",
    "    # flatten last 2 dims then boolean mask\n",
    "    A_flat = tf.reshape(A, (tf.shape(A)[0], -1))           # (B, N*N)\n",
    "    mask_flat = tf.reshape(mask, (-1,))                    # (N*N,)\n",
    "    return tf.boolean_mask(A_flat, mask_flat, axis=1)      # (B, N*(N-1)/2)\n",
    "\n",
    "def fc_generation(\n",
    "    cnn_model: tf.keras.Model,\n",
    "    window_batch: tf.Tensor,\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    window_batch: (B, N, L, 1) where N=116, L=20\n",
    "    Returns: (B, 6670) vectorized dynamic FC feature\n",
    "    \"\"\"\n",
    "    # CNN features: (B, N, d, 1) if you used filters=1 final conv (as in paper)\n",
    "    feat = cnn_model(window_batch, training=False)\n",
    "\n",
    "    # squeeze last channel -> (B, N, d)\n",
    "    feat = tf.squeeze(feat, axis=-1)\n",
    "\n",
    "    # Bilinear FC: A = feat @ feat^T -> (B, N, N)\n",
    "    A = tf.matmul(feat, feat, transpose_b=True)\n",
    "\n",
    "    # Flatten (upper triangle) -> (B, 6670)\n",
    "    f = upper_triangle_vectorize(A)\n",
    "\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85c5e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def subject_dfc_sequence(\n",
    "    cnn_model: tf.keras.Model,\n",
    "    windows_TNL: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    windows_TNL: (T, N, L)\n",
    "    Returns: (T, 6670)\n",
    "    \"\"\"\n",
    "    # Add channel dim: (T, N, L, 1)\n",
    "    x = windows_TNL[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "    # Treat each window as a batch element (B=T)\n",
    "    f = fc_generation(cnn_model, tf.convert_to_tensor(x))\n",
    "    return f.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e83c75",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "We verify output shapes and FC symmetry on a single subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "713e3303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows: (8, 116, 20)\n",
      "dfc seq: (8, 6670)\n"
     ]
    }
   ],
   "source": [
    "cnn = tdnet_cnn_block(dropout=0.3)\n",
    "\n",
    "# Take 1 subject, 1 window\n",
    "one_subject = df_train.iloc[0]\n",
    "X = load_subject_timeseries(one_subject[\"tc_path\"])\n",
    "X, _ = ensure_N_by_M(X, expected_N=116)\n",
    "X = truncate_to_paper_length(X, one_subject[\"site\"], drop_if_shorter=True)\n",
    "\n",
    "res = partition_time_series(X, L=20, step=20, time_axis=-1, return_metadata=True)\n",
    "windows = res.windows  # (T, 116, 20)\n",
    "\n",
    "seq = subject_dfc_sequence(cnn, windows)\n",
    "print(\"windows:\", windows.shape)     # (T, 116, 20)\n",
    "print(\"dfc seq:\", seq.shape)         # (T, 6670)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "950b01a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN output shape: (1, 116, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "w = windows[0][np.newaxis, ..., np.newaxis].astype(np.float32)  # (1,116,20,1)\n",
    "print(\"CNN output shape:\", cnn(w, training=False).shape)         # expect (1,116,16,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5931cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max symmetry error: 0.0\n"
     ]
    }
   ],
   "source": [
    "feat = tf.squeeze(cnn(w, training=False), axis=-1)              # (1,116,16)\n",
    "A = tf.matmul(feat, feat, transpose_b=True)                     # (1,116,116)\n",
    "sym_err = tf.reduce_max(tf.abs(A - tf.transpose(A, perm=[0,2,1]))).numpy()\n",
    "print(\"max symmetry error:\", sym_err)  # should be ~0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
