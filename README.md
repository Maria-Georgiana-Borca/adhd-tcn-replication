# ADHD-TCN Replication Study

This repository contains a replication of the Temporal Convolutional Network (TCN) approach for ADHD classification using dynamic functional connectivity (DFC) from the ADHD-200 dataset.

## Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Setup](#setup)
  - [Option 1: Docker (Recommended)](#option-1-docker-recommended)
  - [Option 2: Local Installation](#option-2-local-installation)
- [Data Download](#data-download)
- [Running the Experiments](#running-the-experiments)
- [Results](#results)
- [Artifacts](#artifacts)

## Overview

This project implements an end-to-end deep learning pipeline for ADHD classification using:
- **Data**: ADHD-200 dataset (Athena-preprocessed AAL ROI time series)
- **Architecture**: Temporal Convolutional Network (TCN) with trainable dynamic functional connectivity generation
- **Sites**: KKI, NeuroIMAGE, NYU, OHSU, Peking (multiple sites)
- **Task**: Binary classification (ADHD vs Control)

The pipeline consists of:
1. **Data Preparation** - Processing raw AAL time series and phenotypic data
2. **Windowing** - Creating non-overlapping temporal windows from time series
3. **Model Training** - End-to-end DFC-TCN training with joint FC generation and temporal modeling

## Project Structure

```
adhd-tcn-replication/
├── data/
│   ├── raw/                    # Raw ADHD-200 dataset (not included in repo)
│   │   ├── aal_tcs/           # AAL time series per site
│   │   └── phenotypic/        # Phenotypic data per site
│   ├── processed/             # Processed data (generated by notebooks)
│   │   ├── windows/           # Windowed time series
│   │   ├── dfc_sequences/     # DFC sequences (if applicable)
│   │   └── *.csv              # Manifests and subject lists
│   └── interim/               # Intermediate processing files
├── notebooks/
│   ├── 01_data_preparation.ipynb    # Data loading and preprocessing
│   ├── 02_data_windowing.ipynb      # Time series windowing
│   └── 03_model_dfc_tcn.ipynb       # Model training and evaluation
├── models/                    # Saved model checkpoints
├── artifacts/                 # Training artifacts
│   └── models/               # Per-site trained models
├── results/                   # Training history and evaluation metrics
├── requirements.txt           # Python dependencies
├── Dockerfile                 # Docker container configuration
└── README.md                  # This file
```

## Requirements

### System Requirements
- **Memory**: At least 8GB RAM (16GB recommended)
- **Storage**: ~10GB for dataset + 5GB for processed data
- **GPU**: Optional but recommended (CUDA-compatible for TensorFlow/PyTorch)

### Software Requirements
- **Docker**: 20.10+ (for containerized setup)
- **Python**: 3.13 (for local setup)
- **Jupyter Lab**: Included in dependencies

## Setup

### Option 1: Docker (Recommended)

Docker provides an isolated environment with all dependencies pre-installed.

#### Quick Start (Easiest)

Use the provided quick-start script for automated setup:

```bash
./quick-start.sh
```

This script will:
- Check if Docker is installed
- Create necessary directories
- Build the Docker image
- Start Jupyter Lab automatically

#### Using Docker Compose

For easier container management:

```bash
# Build and start the container
docker-compose up

# Build and start in detached mode (background)
docker-compose up -d

# Stop the container
docker-compose down
```

#### Manual Docker Setup

**1. Build the Docker Image:**

```bash
docker build -t adhd-tcn-replication .
```

**2. Run the Container:**

For Jupyter Lab (interactive notebooks):

```bash
docker run -it --rm \
  -p 8888:8888 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/results:/app/results \
  -v $(pwd)/artifacts:/app/artifacts \
  -v $(pwd)/notebooks:/app/notebooks \
  adhd-tcn-replication
```

With GPU support (if available):

```bash
docker run -it --rm --gpus all \
  -p 8888:8888 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/results:/app/results \
  -v $(pwd)/artifacts:/app/artifacts \
  -v $(pwd)/notebooks:/app/notebooks \
  adhd-tcn-replication
```

**3. Access Jupyter Lab:**

After starting the container, open your browser and navigate to:
```
http://localhost:8888
```

The Jupyter Lab interface will be available with no password required.

### Option 2: Local Installation

#### 1. Clone the Repository

```bash
git clone <repository-url>
cd adhd-tcn-replication
```

#### 2. Create Virtual Environment

```bash
python3.13 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

#### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

#### 4. Start Jupyter Lab

```bash
jupyter lab
```

## Data Download

The ADHD-200 dataset is not included in this repository due to size constraints. Follow these steps to download and prepare the data:

### 1. Download Dataset

Visit the ADHD-200 website and download the Athena-preprocessed data:
- **Website**: https://www.nitrc.org/frs/?group_id=383
- **Required files**:
  - `ADHD200_AAL_TCs_filtfix.tar` (training data AAL time series)
  - `ADHD200_training_pheno.tar` (training phenotypic data)
  - Optionally: `ADHD200_AAL_TCs_TestRelease.tar` and `ADHD200_testing_pheno.tar` (test data)

### 2. Extract Data

Extract the downloaded files into the `data/raw/` directory:

```bash
# Create directories
mkdir -p data/raw/aal_tcs
mkdir -p data/raw/phenotypic

# Extract AAL time series
cd data/raw/aal_tcs
tar -xvf ~/Downloads/ADHD200_AAL_TCs_filtfix.tar

# Extract phenotypic data
cd ../phenotypic
tar -xvf ~/Downloads/ADHD200_training_pheno.tar
```

### 3. Verify Directory Structure

After extraction, your `data/raw/` directory should look like:

```
data/raw/
├── aal_tcs/
│   ├── KKI/
│   ├── NeuroIMAGE/
│   ├── NYU/
│   ├── OHSU/
│   ├── Peking_1/
│   ├── Peking_2/
│   ├── Peking_3/
│   └── ...
└── phenotypic/
    ├── KKI/
    ├── NeuroIMAGE/
    ├── NYU/
    └── ...
```

**Note**: If using Docker, ensure the data is downloaded before starting the container, as the volumes are mounted from your host machine.

## Running the Experiments

The experimental pipeline consists of three sequential notebooks:

### 1. Data Preparation

**Notebook**: `notebooks/01_data_preparation.ipynb`

This notebook:
- Loads raw AAL time series from each site
- Merges phenotypic information
- Filters subjects by quality criteria
- Creates train/validation splits
- Saves processed data to `data/processed/`

**To run**:
1. Open Jupyter Lab (see Setup section)
2. Navigate to `notebooks/01_data_preparation.ipynb`
3. Run all cells (Cell → Run All)

**Outputs**:
- `data/processed/subjects_train.csv`
- `data/processed/subjects_train_split_paper.csv`
- `data/processed/subjects_val_split_paper.csv`

### 2. Data Windowing

**Notebook**: `notebooks/02_data_windowing.ipynb`

This notebook:
- Creates non-overlapping temporal windows (default: 20 timepoints per window)
- Saves windowed tensors for efficient loading during training
- Creates manifest files

**To run**:
1. Ensure notebook 01 has completed successfully
2. Open `notebooks/02_data_windowing.ipynb`
3. Run all cells

**Outputs**:
- `data/processed/windows/train/<SITE>/<SUBJECT_ID>.npy`
- `data/processed/windows/val/<SITE>/<SUBJECT_ID>.npy`
- `data/processed/windows_manifest.csv`

### 3. Model Training and Evaluation

**Notebook**: `notebooks/03_model_dfc_tcn.ipynb`

This notebook:
- Implements the end-to-end DFC-TCN architecture
- Trains the model with joint FC generation and TCN classification
- Evaluates on validation set
- Saves model checkpoints and training history

**To run**:
1. Ensure notebooks 01 and 02 have completed successfully
2. Open `notebooks/03_model_dfc_tcn.ipynb`
3. Run all cells

**Training parameters** (configurable in notebook):
- Batch size: 16
- Learning rate: 1e-3
- Epochs: 100 (with early stopping)
- Dropout: 0.3

**Outputs**:
- `models/dfc_tcn_best.keras` (best model checkpoint)
- `results/history.csv` (training history)
- `results/val_predictions.csv` (validation predictions)

## Results

Training results and evaluation metrics are saved in the `results/` directory:

- **Training history**: `results/history_tdnet_T12.csv`
  - Contains loss, accuracy, validation loss, and validation accuracy per epoch

- **Model checkpoints**: Multiple seed runs available in `models/`
  - `tdnet_T12_seed42.weights.h5`
  - `tdnet_T12_seed123.weights.h5`
  - `tdnet_T12_seed456.weights.h5`
  - `tdnet_T12_seed789.weights.h5`
  - `tdnet_T12_seed2024.weights.h5`

### Loading Pre-trained Models

```python
import tensorflow as tf

# Load a pre-trained model
model = tf.keras.models.load_model('models/tdnet_T12_best.keras')

# Use for inference
predictions = model.predict(test_data)
```

## Troubleshooting

### Common Issues

**1. Out of Memory Error**
- Reduce `BATCH_SIZE` in notebook 03
- Close other applications
- Use a machine with more RAM

**2. Data Not Found**
- Verify data is downloaded and extracted correctly
- Check paths in notebooks match your directory structure
- Ensure Docker volumes are mounted correctly

**3. Jupyter Lab Access Issues**
- Verify port 8888 is not in use by another application
- Check firewall settings
- Try accessing via `127.0.0.1:8888` instead of `localhost:8888`

**4. GPU Not Detected**
- Install CUDA and cuDNN (for local setup)
- Use `--gpus all` flag with Docker
- Verify GPU with: `python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"`

## Citation

If you use this code or approach, please cite the original ADHD-200 dataset and relevant papers:

```
ADHD-200 Consortium. (2012). The ADHD-200 Consortium: A Model to Advance the 
Translational Potential of Neuroimaging in Clinical Neuroscience. 
Frontiers in Systems Neuroscience, 6, 62.
```

## License

This is an academic project for educational purposes. Please respect the ADHD-200 data usage agreement.

## Contact

For questions or issues, please open an issue in the GitHub repository.
